{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"data/movies_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# only keep the needed columns\n",
    "data = data_raw[[\"overview\",\"title\",\"genres\"]]\n",
    "data = data[~data['overview'].isna()]\n",
    "# create a mask indication where a genre value exists\n",
    "has_genres_mask = data['genres'] != \"[]\"\n",
    "genres = data['genres'][has_genres_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AST allows us to evaluate the string list in each genre entry.\n",
    "basically creates a list from a string with list content.\n",
    "\"\"\"\n",
    "import ast\n",
    "\n",
    "def make_labels(strings):\n",
    "    evaluated_string = ast.literal_eval(strings)\n",
    "    return [g['name'] for g in evaluated_string]\n",
    "\n",
    "genres_list = genres.apply(make_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\"\"\"\n",
    "the MultiLabelBinarizer simply looks through all entries in a list and creates list containing unique labels.\n",
    "\"\"\"\n",
    "labeler = MultiLabelBinarizer()\n",
    "labeler.fit(genres_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(labeler, \"model/class_labler.joblib\")\n",
    "print(labeler.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for the next steps its really important to make sure both entries are of string type\n",
    "pre_X1 = data['title'][has_genres_mask].astype(dtype=\"str\")\n",
    "pre_X2 = data['overview'][has_genres_mask].astype(dtype=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "now we have a binary list representing the genres.\n",
    "this can be used directly in the training\n",
    "\"\"\"\n",
    "y = labeler.transform(genres_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Just checking the size of all entries, Now we are sure everything is aligned correctly\n",
    "\"\"\"\n",
    "print(len(y))\n",
    "print(len(pre_X1))\n",
    "print(len(pre_X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The maximum number of words to be used.\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100 # This is a fixed value, in this case\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(pre_X1 + pre_X2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "dump(tokenizer, \"model/tokenizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def max_len(array_):\n",
    "    max_l = 0\n",
    "    for x in array_:\n",
    "        if len(x) > max_l:\n",
    "            max_l = len(x)\n",
    "    return max_l\n",
    "X1 = tokenizer.texts_to_sequences(pre_X1)\n",
    "X2 = tokenizer.texts_to_sequences(pre_X2)\n",
    "\n",
    "X1_max_len = max_len(X1)\n",
    "X2_max_len = max_len(X2)\n",
    "\n",
    "X1 = keras.preprocessing.sequence.pad_sequences(X1, maxlen=X1_max_len)\n",
    "X2 = keras.preprocessing.sequence.pad_sequences(X2, maxlen=X2_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this section attempted to split the data into training and validation sets\n",
    "However, this was scrapped in favor of letting the fit function\n",
    "deal with the validation split\n",
    "\n",
    "However, this is used in generating a dataset for the evaluation\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test = train_test_split(X1, shuffle = False)\n",
    "X2_train, X2_test = train_test_split(X2, shuffle = False)\n",
    "Y_train, Y_test = train_test_split(y, shuffle = False)\n",
    "\n",
    "# print(X1_train.shape,Y_train.shape)\n",
    "# print(X1_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load the classifier we constructed\n",
    "\"\"\"\n",
    "from model import GenreClassifier\n",
    "\n",
    "model = GenreClassifier(len(labeler.classes_), MAX_NB_WORDS, X1_max_len, X2_max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train the classifier\n",
    "the specified batch size requires lots of memory, consider reducing the value on first run\n",
    "\"\"\"\n",
    "EPOCHS = 33\n",
    "BATCH_SIZE = 1024\n",
    "history = model.fit(X1, X2, y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"model/simple_text_classifier_33.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X1_test, X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The confusion matrix below allows us to see how the predictions compare to the ground truth,\n",
    "the labels are omitted from this plot to avoid clutter.\n",
    "\"\"\"\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(matrix)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}